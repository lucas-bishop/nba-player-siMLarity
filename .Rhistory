fav_wplive_prob=ifelse(win_prob1 > win_prob2, win_prob1, win_prob2)) %>%
select(season, date, team1, team2, score1, score2, fav_538_won, fav_538_prob, fav_wplive_won, fav_wplive_prob)
View(wl_season)
# now want to join in the wl_season df
favorite_win_prob <- favorite_win_prob %>%
inner_join(., wl_season, by=c("team1"="team", "season")) %>%
inner_join(., wl_season, by=c("team2"="team", "season")) %>%
mutate(win_prob1=map2_dbl(current_avg.x, current_avg.y, get_wp),
win_prob2=1-win_prob1,
fav_wpcurrent_won=ifelse(win_prob1 > win_prob2, score1 > score2, score2 > score1),
fav_wpcurrent_prob=ifelse(win_prob1 > win_prob2, win_prob1, win_prob2),
win_prob1=map2_dbl(prev_avg.x, prev_avg.y, get_wp),
win_prob2=1-win_prob1,
fav_wpprev_won=ifelse(win_prob1 > win_prob2, score1 > score2, score2 > score1),
fav_wpprev_prob=ifelse(win_prob1 > win_prob2, win_prob1, win_prob2)
) %>%
select(season, date, team1, team2, score1, score2, fav_538_won, fav_538_prob, fav_wplive_won, fav_wplive_prob, fav_wpcurrent_won, fav_wpcurrent_prob, fav_wpprev_won, fav_wpprev_prob)
View(favorite_win_prob)
tail(favorite_win_prob)
tidy_win_prob <- favorite_win_prob %>%
mutate(FiveThirtyEight=paste(fav_538_won, fav_538_prob, sep="_"),
wplive=paste(fav_wplive_won, fav_wplive_prob, sep="_"),
wpcurrent=paste(fav_wpcurrent_won, fav_wpcurrent_prob, sep="_"),
wpprev=paste(fav_wpprev_won, fav_wpprev_prob, sep="_")) %>%
select(-starts_with("fav")) %>%
gather(model, won_prob, fte, wplive, wpcurrent, wpprev) %>%
separate(won_prob, into=c("won", "prob"), sep="_", convert=TRUE)
tidy_win_prob <- favorite_win_prob %>%
mutate(FiveThirtyEight=paste(fav_538_won, fav_538_prob, sep="_"),
wplive=paste(fav_wplive_won, fav_wplive_prob, sep="_"),
wpcurrent=paste(fav_wpcurrent_won, fav_wpcurrent_prob, sep="_"),
wpprev=paste(fav_wpprev_won, fav_wpprev_prob, sep="_")) %>%
select(-starts_with("fav")) %>%
gather(model, won_prob, FiveThirtyEight, wplive, wpcurrent, wpprev) %>%
separate(won_prob, into=c("won", "prob"), sep="_", convert=TRUE)
View(tidy_win_prob)
overall_win_prob <- tidy_win_prob %>% group_by(model) %>%
summarize(mean=mean(won))
View(overall_win_prob)
tidy_win_prob %>%
group_by(season, model) %>%
summarize(fraction_favorite_won = mean(won)) %>%
ungroup() %>%
ggplot(aes(x=season, y=fraction_favorite_won, group=model, color=model)) +
geom_hline(data=overall_win_prob, aes(yintercept=mean, group=model, color=model)) +
geom_line() +
theme_classic() +
coord_cartesian(ylim=c(0,1)) +
labs(x="Season", y="Fraction of games favorite won",
title="The Winning Percentage model can out perform the 538 ELO model if it uses end of\nseason winning averages") +
scale_color_manual(name=NULL,
breaks=c("fte", "wpcurrent", "wplive", "wpprev"),
labels=c("538", "WP Curent", "WP Live", "WP Previous"),
values=wes_palette("Darjeeling2"))
tidy_win_prob %>%
group_by(season, model) %>%
summarize(fraction_favorite_won = mean(won)) %>%
ungroup() %>%
ggplot(aes(x=season, y=fraction_favorite_won, group=model, color=model)) +
geom_hline(data=overall_win_prob, aes(yintercept=mean, group=model, color=model)) +
geom_line() +
theme_classic() +
coord_cartesian(ylim=c(0,1)) +
labs(x="Season", y="Fraction of games favorite won",
title="The Winning Percentage model can out perform the 538 ELO model if it uses end of\nseason winning averages") +
scale_color_brewer(name=NULL,
breaks=c("fte", "wpcurrent", "wplive", "wpprev"),
labels=c("538", "WP Curent", "WP Live", "WP Previous"),
palette = "Dark2")
tidy_win_prob %>%
group_by(season, model) %>%
summarize(fraction_favorite_won = mean(won)) %>%
ungroup() %>%
ggplot(aes(x=season, y=fraction_favorite_won, group=model, color=model)) +
geom_hline(data=overall_win_prob, aes(yintercept=mean, group=model, color=model)) +
geom_line() +
theme_classic() +
coord_cartesian(ylim=c(0,1)) +
labs(x="Season", y="Fraction of games favorite won",
title="The Winning Percentage model can out perform the 538\n ELO model if it uses end of\nseason winning averages") +
scale_color_brewer(name=NULL,
breaks=c("", "wpcurrent", "wplive", "wpprev"),
labels=c("538", "WP Curent", "WP Live", "WP Previous"),
palette = "Dark2")
tidy_win_prob %>%
group_by(season, model) %>%
summarize(fraction_favorite_won = mean(won)) %>%
ungroup() %>%
ggplot(aes(x=season, y=fraction_favorite_won, group=model, color=model)) +
geom_hline(data=overall_win_prob, aes(yintercept=mean, group=model, color=model)) +
geom_line() +
theme_classic() +
coord_cartesian(ylim=c(0,1)) +
labs(x="Season", y="Fraction of games favorite won",
title="The Winning Percentage model can out perform the 538\n ELO model if it uses end of season winning averages") +
scale_color_brewer(name=NULL,
breaks=c("", "wpcurrent", "wplive", "wpprev"),
labels=c("538", "WP Curent", "WP Live", "WP Previous"),
palette = "Dark2")
tidy_win_prob %>%
group_by(season, model) %>%
summarize(fraction_favorite_won = mean(won)) %>%
ungroup() %>%
ggplot(aes(x=season, y=fraction_favorite_won, group=model, color=model)) +
geom_hline(data=overall_win_prob, aes(yintercept=mean, group=model, color=model)) +
geom_line() +
theme_classic() +
coord_cartesian(ylim=c(0,1)) +
labs(x="Season", y="Fraction of games favorite won",
title="The Winning Percentage model can out perform the 538\nELO model if it uses end of season winning averages") +
scale_color_brewer(name=NULL,
breaks=c("", "wpcurrent", "wplive", "wpprev"),
labels=c("538", "WP Curent", "WP Live", "WP Previous"),
palette = "Dark2")
tidy_win_prob %>%
group_by(season, model) %>%
summarize(fraction_favorite_won = mean(won)) %>%
ungroup() %>%
ggplot(aes(x=season, y=fraction_favorite_won, group=model, color=model)) +
geom_hline(data=overall_win_prob, aes(yintercept=mean, group=model, color=model)) +
geom_line() +
theme_classic() +
coord_cartesian(ylim=c(0,1)) +
labs(x="Season", y="Fraction of games favorite won",
title="The Winning Percentage model can out perform the 538\nELO model if it uses end of season winning averages") +
scale_color_brewer(name=NULL,
breaks=c("FiveThirtyEight", "wpcurrent", "wplive", "wpprev"),
labels=c("538", "WP Curent", "WP Live", "WP Previous"),
palette = "Dark2")
# now plot the observed versus expected
tidy_win_prob %>%
mutate(prob = round(prob, digits=2)) %>%
group_by(prob, model) %>%
summarize(games = n(),
wins = sum(won),
observed = wins / games) %>%
ggplot(aes(x=prob,  y=observed, group=model, color=model)) +
geom_abline(aes(intercept=0, slope=1), color="gray") +
geom_line() +
theme_classic() +
scale_color_manual(name=NULL,
breaks=c("FiveThirtyEight", "wpcurrent", "wplive", "wpprev"),
labels=c("538", "WP Curent", "WP Live", "WP Previous"),
values=wes_palette("Darjeeling2")) +
labs(x="Predicted Win Probability", y="Observed Win Probability",
title="The 538 and WP Current models generate more reliable win probabilities than the\nWP Live or Previous models",
subtitle="All data since 1995 season")
# now plot the observed versus expected
tidy_win_prob %>%
mutate(prob = round(prob, digits=2)) %>%
group_by(prob, model) %>%
summarize(games = n(),
wins = sum(won),
observed = wins / games) %>%
ggplot(aes(x=prob,  y=observed, group=model, color=model)) +
geom_abline(aes(intercept=0, slope=1), color="gray") +
geom_line() +
theme_classic() +
scale_color_brewer(name=NULL,
breaks=c("FiveThirtyEight", "wpcurrent", "wplive", "wpprev"),
labels=c("538", "WP Curent", "WP Live", "WP Previous"),
palette = "Dark2") +
labs(x="Predicted Win Probability", y="Observed Win Probability",
title="The 538 and WP Current models generate more reliable win probabilities than the\nWP Live or Previous models",
subtitle="All data since 1995 season")
#at time of writing this code, the NBA season has been postponed due to coronavirus.
#Since the games are still being logged on our data source while they are postponed, we need to make a variable for the last normal day of NBA schedule
#if you use current date, postponed games will screw up downstream analysis
last_games_day <- as.Date(c('2020-03-10'))
# Load and format basketball games that have already been played. I will focus only on post 1995 stats
all_game_data <- read_csv(file="https://projects.fivethirtyeight.com/nba-model/nba_elo.csv",
col_types=cols(date = col_date(),
date=col_date(),
team1=col_character(),
team2=col_character(),
season=col_integer(),
score1=col_integer(),
score2=col_integer())
) %>% filter(date < last_games_day, season >= 1995) %>%
select(-'carm-elo1_pre', -'carm-elo1_post', -'carm-elo2_pre', -'carm-elo2_post', -'carm-elo_prob1',
-'carm-elo_prob2', -'raptor1_pre', -'raptor2_pre', -'raptor_prob1', -'raptor_prob2')
# add new columns that show the favorite and if they won based on ELO model. I will do since 1995 since I have been baskterball fan since then
favorite_win_prob <- all_game_data %>%
mutate(fav_538_won=ifelse(elo_prob1>elo_prob2, score1 > score2, score2 > score1),
fav_538_prob=ifelse(elo_prob1>elo_prob2, elo_prob1, elo_prob2)) %>%
select(season, date, team1, team2, score1, score2, fav_538_won, fav_538_prob)
overall_win_prob <- mean(favorite_win_prob$fav_538_won, na.rm = TRUE)
# in history of games played, plot the fraction of games that the favorite won
favorite_win_prob %>%
group_by(season) %>%
summarize(fraction_favorite_won = mean(fav_538_won)) %>%
ggplot(aes(x=season, y=fraction_favorite_won)) +
geom_hline(aes(yintercept=overall_win_prob), color="lightgray") +
geom_line() +
theme_classic() +
coord_cartesian(ylim=c(0,1)) +
labs(x="Season", y="Proportion of games that favorite won",
title="The 538 model does a better than average job\n of predicting the winner of NBA games")
#plot the observed versus expected fraction of games won
all_predicted_observed <- favorite_win_prob %>%
mutate(fav_538_prob = round(fav_538_prob, digits=2)) %>%
group_by(fav_538_prob) %>%
summarize(games = n(),
wins = sum(fav_538_won, na.rm = TRUE),
observed = wins / games)
binomial_df <- all_predicted_observed %>%
mutate(prob = fav_538_prob) %>%
group_by(fav_538_prob) %>%
nest() %>%
## now have a data frame of data frames basically, want to know probability of success ad confidence intervals for each row(df) in our larger df
mutate(binomial = map(data, function(df)
tidy(binom.test(x=as.integer(df$games * df$prob),
n=df$games),
p=df$prob)
)) %>%
unnest(cols = c(data, binomial)) %>%
##now have a 1 x 4 tibble from all_predicted_observed and a binomial fit 1 x 8 df joined together grouped by each 538_prob
select(fav_538_prob, games, wins, observed, conf.low, conf.high)
binomial_df %>%
ggplot(aes(x=fav_538_prob, y=observed)) +
geom_ribbon(aes(ymin=conf.low, ymax=conf.high), fill="lightblue") +
geom_abline(aes(intercept=0, slope=1), color="darkgray") +
geom_point() +
theme_classic() +
coord_cartesian(ylim=c(0,1)) +
labs(x="Predicted Probability of Winning",
y="Observed Probability of Winning",
title="The 538 model underpredicts the true ability of the favorite\n to win",
subtitle="All games from 1995 season to present")
library(tidyverse)
candy_data <- read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/candy-power-ranking/candy-data.csv",
col_types="clllllllllddd")
View(candy_data)
candy_data %>%	# this is piping the dataframe we
filter(pluribus)
candy_data %>%	#
filter(pluribus)
candy_data %>%	#
filter(pluribus) %>%
candy_data %>%	#
filter(pluribus)  #
pivot_longer(
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer") %>% #
filter(answer) %>% #
group_by(type) %>% #
summarize(total = sum(answer))
candy_data
filter(pluribus)   #
pivot_longer(
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer")#
filter(answer)
group_by(type)
summarize(total = sum(answer)) #
candy_data %>% #
filter(pluribus) %>%   # filters the candy_data df to only include bite size candy
pivot_longer(
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer"
candy_data %>% #
filter(pluribus) %>%   # filters the candy_data df to only include bite size candy
pivot_longer(
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer")
candy_data %>% #
filter(pluribus) %>%   # filters the candy_data df to only include bite size candy
pivot_longer(
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer")
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer") %>%
select(type == chocolate) %>%
filter(answer == TRUE)
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer") %>%
select('type' == chocolate) %>%
filter(answer == TRUE)
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer") %>%
select(type == 'chocolate') %>%
filter(answer == TRUE)
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer") %>%
filter(type == 'chocolate') %>%
filter(answer == TRUE)
candy_data %>% #
filter(pluribus) %>%   # filters the candy_data df to only include bite size candy
pivot_longer(
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer") %>% # turns a wide data frame into a long one based on the keys we give it
filter(answer)
candy_data %>% #
filter(pluribus) %>%   # filters the candy_data df to only include bite size candy
pivot_longer(
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer") %>% # turns a wide data frame into a long one based on the keys we give it
filter(answer) %>% # focuses the df further to only show what the candy is, rather than what it is not
group_by(type)
candy_data %>% #
filter(pluribus) %>%   # filters the candy_data df to only include bite size candy
pivot_longer(
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer") %>% # turns a wide data frame into a long one based on the keys we give it
filter(answer) %>% # focuses the df further to only show what the candy is, rather than what it is not
group_by(type) %>% #
summarize(total = sum(answer))
candy_data %>% #
filter(pluribus) %>%   # filters the candy_data df to only include bite size candy
pivot_longer(
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer") %>% # turns a wide data frame into a long one based on the keys we give it
filter(answer) %>% # focuses the df further to only show what the candy is, rather than what it is not
#group_by(type) %>% #
summarize(total = sum(answer))
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer") %>% # turns a wide data frame into a long one based on the keys we give it
filter(answer) %>% # focuses the df further to only show what the candy is, rather than what it is not
group_by(type) %>% #
summarize(total = sum(answer)) %>% #
coord_cartesian(ylim=c(0,100)) +
filter(answer)
candy_data %>% #
filter(pluribus) %>%   # filters the candy_data df to only include bite size candy
pivot_longer(
cols=c(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus),
names_to="type",
values_to="answer") %>% # turns a wide data frame into a long one based on the keys we give it
filter(answer) %>% # focuses the df further to only show what the candy is, rather than what it is not
group_by(type) %>% #
summarize(total = sum(answer))
# Excercise 2
pluribus_data <- candy_data %>%
filter(pluribus & (chocolate | fruity)) %>%
pivot_longer(cols=c(chocolate, fruity), names_to="type", values_to="answer") %>%
filter(answer)
View(pluribus_data)
pluribus_data %>%
group_by(type) %>%
summarize(median=median(winpercent), IQR=IQR(winpercent), N=n())
pluribus_data %>%
ggplot(aes(x=type, y=winpercent, color=hard)) +
geom_jitter(width=0.1) +
scale_x_discrete(breaks=c("chocolate", "fruity"), labels=c("Chocolate", "Fruity")) +
coord_cartesian(ylim=c(0,100)) +
labs(y="Contests won (%)", x=NULL, title="Bite-sized candies containing chocolate are preferred to candy without") +
theme_classic()
View(pluribus_data)
# Excercise 2
pluribus_data <- candy_data %>%
filter(pluribus & (chocolate | fruity)) %>%
pivot_longer(cols=c(chocolate, fruity), names_to="type", values_to="answer") %>%
filter(answer)
View(pluribus_data)
# Excercise 2
pluribus_data <- candy_data %>%
pivot_longer(cols=c(chocolate, fruity), names_to="type", values_to="answer") %>%
filter(pluribus & (chocolate | fruity)) %>%
filter(answer)
View(candy_data)
View(pluribus_data)
pluribus_data %>%
group_by(type) %>%
summarize(median=median(winpercent), IQR=IQR(winpercent), N=n())
# Excercise 3
pluribus_data %>%
ggplot(aes(x=type, y=pricepercent, color=chocolate)) +
geom_jitter(width=0.1) +
scale_x_discrete(breaks=c("chocolate", "fruity"), labels=c("Chocolate", "Fruity")) +
coord_cartesian(ylim=c(0,1)) +
labs(y="Contests won (%)", x=NULL, title="Bite-sized candies containing chocolate are preferred to candy without") +
theme_classic()
# Excercise 3
pluribus_data %>% filter(type == 'chocolate') %>%
ggplot(aes(x=type, y=pricepercent, color=chocolate)) +
geom_jitter(width=0.1) +
scale_x_discrete(breaks=c("chocolate", "fruity"), labels=c("Chocolate", "Fruity")) +
coord_cartesian(ylim=c(0,1)) +
labs(y="Contests won (%)", x=NULL, title="Bite-sized candies containing chocolate are preferred to candy without") +
theme_classic()
# Excercise 3
pluribus_data %>%
ggplot(aes(x=type, y=pricepercent, color=type)) +
geom_jitter(width=0.1) +
scale_x_discrete(breaks=c("chocolate", "fruity"), labels=c("Chocolate", "Fruity")) +
coord_cartesian(ylim=c(0,1)) +
labs(y="Contests won (%)", x=NULL, title="Bite-sized candies containing chocolate are preferred to candy without") +
theme_classic()
# Excercise 3
pluribus_data %>%
ggplot(aes(x=type, y=pricepercent, color=type)) +
geom_jitter(width=0.1) +
scale_x_discrete(breaks=c("chocolate", "fruity"), labels=c("Chocolate", "Fruity")) +
coord_cartesian(ylim=c(0,1)) +
labs(y="price percent (%)", x=NULL, title="Bite-sized candies containing chocolate are preferred to candy without") +
theme_classic()
library(tidyverse)
library(Hmisc)
library(readxl)
# load data from internet sources
policy_df <- read_csv('https://s3-us-west-2.amazonaws.com/campaign-zero-use-of-force/data/force.csv')
src <- 'https://mappingpoliceviolence.org/s/MPVDatasetDownload.xlsx'
lcl <- basename(src)
download.file(url = src, destfile = lcl)
use_of_force_df <- read_xlsx(lcl, sheet = '2013-2019 Killings by PD')
src <- 'https://mappingpoliceviolence.org/s/MPVDatasetDownload.xlsx'
lcl <- basename(src)
download.file(url = src, destfile = lcl)
use_of_force_df <- read_xlsx(lcl, sheet = '2013-2019 Killings by PD')
library(Hmisc)
src1 <- 'https://mappingpoliceviolence.org/s/MPVDatasetDownload.xlsx'
lcl <- basename(src1)
download.file(url = src1, destfile = lcl)
use_of_force_df <- read_xlsx(lcl, sheet = '2013-2019 Killings by PD')
ls
install.packages(c('here', 'testthat'))
library(here)
# extra pages for misc df have only qualified players, qualifications found here: https://basketball.realgm.com/info/glossary
extra1 <- read_html("https://basketball.realgm.com/nba/stats/2019/Misc_Stats/Qualified/dbl_dbl/All/desc/ 1") %>%
html_table("table#table-7945.tablesaw.compact.tablesaw-stack", header = NA, fill = TRUE)
library(tidyverse)
library(rvest)
library(BBmisc)
library(ggrepel)
# this function needs to be tuned to incclude all table elements from each of the 3 pages
# then select the ones we need as nodes
# pull_player_data <- function(url){
#   print(url)
#   read_html(url) %>%
#     html_table("table#table-4978.tablesaw.compact.tablesaw-stack", header = NA, fill = TRUE)
# }
# html_pages <- c(1:3) %>%
#  paste("https://basketball.realgm.com/nba/stats/2019/Misc_Stats/Qualified/dbl_dbl/All/desc/",.)
######
#data#
######
traditional <- read_csv("data/36minutes.csv") %>% select(-badcol)
advanced <- read_csv("data/advancedNBA.csv") %>% select(-badcol)
# should make last df with our above function and map it to the html_pages list:
# extra <- map_dfr(html_pages, pull_player_data)
setwd("~/Documents/NBA player siMLarity")
library(tidyverse)
library(rvest)
library(BBmisc)
library(ggrepel)
# this function needs to be tuned to incclude all table elements from each of the 3 pages
# then select the ones we need as nodes
# pull_player_data <- function(url){
#   print(url)
#   read_html(url) %>%
#     html_table("table#table-4978.tablesaw.compact.tablesaw-stack", header = NA, fill = TRUE)
# }
# html_pages <- c(1:3) %>%
#  paste("https://basketball.realgm.com/nba/stats/2019/Misc_Stats/Qualified/dbl_dbl/All/desc/",.)
######
#data#
######
traditional <- read_csv("data/36minutes.csv") %>% select(-badcol)
advanced <- read_csv("data/advancedNBA.csv") %>% select(-badcol)
# should make last df with our above function and map it to the html_pages list:
# extra <- map_dfr(html_pages, pull_player_data)
# extra pages for misc df have only qualified players, qualifications found here: https://basketball.realgm.com/info/glossary
extra1 <- read_html("https://basketball.realgm.com/nba/stats/2019/Misc_Stats/Qualified/dbl_dbl/All/desc/ 1") %>%
html_table("table#table-7945.tablesaw.compact.tablesaw-stack", header = NA, fill = TRUE)
extra2 <- read_html("https://basketball.realgm.com/nba/stats/2019/Misc_Stats/Qualified/dbl_dbl/All/desc/ 2") %>%
html_table("table#table-4389.tablesaw.compact.tablesaw-stack", header = NA, fill = TRUE)
extra3 <- read_html("https://basketball.realgm.com/nba/stats/2019/Misc_Stats/Qualified/dbl_dbl/All/desc/ 3") %>%
html_table("table#table-9644.tablesaw.compact.tablesaw-stack", header = NA, fill = TRUE)
View(extra1)
# need to get rid of positions with dashes in them too
# get rid of the win share stats that are redundant in misc dataset
misc <- rbind(extra1[[1]], extra2[[1]], extra3[[1]]) %>% select(-'#', -"OWS", -"DWS", -"WS")
View(extra2)
View(extra3)
# need to get rid of positions with dashes in them too
# get rid of the win share stats that are redundant in misc dataset
misc <- rbind(extra1[[13]], extra2[[13]], extra3[[13]]) %>% select(-'#', -"OWS", -"DWS", -"WS")
merged_stats <- inner_join(traditional, advanced) %>% select(-Rk)
## cleanup before misc merge
misc$Player=gsub(",", "", misc$Player, fixed = TRUE)
# Had initally tried to replace all of the special characters in foreign names, but eventually just changed the csv files before reading in
#Now have full data frame with combined data - can add more columns if I find extra exportable stats websites
full_stats <- full_join(merged_stats, misc, by = 'Player') %>%
unique() %>% select(-Team) %>% mutate(MPG = MP / G)
#This full Df will have plenty of NA values, because the misc dataset has qualifiers that filter out irrelevant players based on min, pts, ast, stl, blk, etc.
full_stats_qual <- full_stats %>% drop_na()
#This leaves us with a data frame of players that have data entries in all 65 variables, or 'relevant' players
# now to subset this data for only looking at players that play significant minutes. We decide this. I based it on the average of qualified players.
# could use a rarefaction curve to see what mpg cutoff to rarefy to
cutoff <- 25.5
full_stats_qual <- subset(full_stats_qual, MPG >= cutoff)
## Normalize data? So values with large ranges like pts don't have more weight than small range values like blk/stl
# Use BBmisc::normalize to (for each value) subtract the mean of the feature-range from that value and divide by the SD of the feature
normalized_numeric <- normalize(full_stats_qual, method = "standardize")
View(full_stats_qual)
full_stats_qual %>% select(Player, VORP) %>% sort(decreasing = TRUE)
full_stats_qual %>% select(Player, VORP) %>% sort(VORP, decreasing = TRUE)
full_stats_qual %>% select(Player, VORP)
