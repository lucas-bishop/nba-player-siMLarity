setwd("~/Documents/NBA player siMLarity")
library(tidyverse)
library(rvest)
# this function needs to be tuned to incclude all table elements from each of the 3 pages
# then select the ones we need as nodes
pull_player_data <- function(url){
print(url)
read_html(url) %>%
html_table("table#table-4978.tablesaw.compact.tablesaw-stack", header = NA, fill = TRUE)
}
html_pages <- c(1:3) %>%
paste("https://basketball.realgm.com/nba/stats/2019/Misc_Stats/Qualified/dbl_dbl/All/desc/",.)
traditional <- read_csv("data/36minutes.csv") %>% select(-badcol)
advanced <- read_csv("data/advancedNBA.csv") %>% select(-badcol)
# make last df with our above functions
#extra <- map_dfr(html_pages, pull_player_data)
#check if these table numbers change uponreload: they do, change script to pull all tables
extra1 <- read_html("https://basketball.realgm.com/nba/stats/2019/Misc_Stats/Qualified/dbl_dbl/All/desc/ 1") %>%
html_table("table#table-7945.tablesaw.compact.tablesaw-stack", header = NA, fill = TRUE)
#check if these table numbers change uponreload: they do, change script to pull all tables
extra1 <- read_html("https://basketball.realgm.com/nba/stats/2019/Misc_Stats/Qualified/dbl_dbl/All/desc/ 1") %>%
html_table("table#table-7945.tablesaw.compact.tablesaw-stack", header = NA, fill = TRUE)
extra2 <- read_html("https://basketball.realgm.com/nba/stats/2019/Misc_Stats/Qualified/dbl_dbl/All/desc/ 2") %>%
html_table("table#table-4389.tablesaw.compact.tablesaw-stack", header = NA, fill = TRUE)
extra3 <- read_html("https://basketball.realgm.com/nba/stats/2019/Misc_Stats/Qualified/dbl_dbl/All/desc/ 3") %>%
html_table("table#table-9644.tablesaw.compact.tablesaw-stack", header = NA, fill = TRUE)
# get rid of the win share stats that are redundant in misc dataset
misc <- rbind(extra1[[1]], extra2[[1]], extra3[[1]]) %>% select(-'#', -"OWS", -"DWS", -"WS")
merged_stats <- inner_join(traditional, advanced) %>% select(-Rk)
misc$Player=gsub(",", "", misc$Player, fixed = TRUE)
#Now have full data frame with combined data - can add more columns if I find extra exportable stats websites
full_stats <- full_join(merged_stats, misc, by = 'Player') %>%
unique() %>% select(-Team) %>% mutate(MPG = MP / G)
#This full Df will have plenty of NA values, because the misc dataset has qualifiers that filter out irrelevant players based on min, pts, ast, stl, blk, etc.
full_stats_qual <- full_stats %>% drop_na()
View(full_stats_qual)
## Normalize data? So values with large ranges like pts don't have more weight than small range values like blk/stl
cutoff <- 28.5
full_stats_qual <- subset(full_stats_qual, MPG >= cutoff)
#This leaves us with a data frame of players that have data entries in all 65 variables, or 'relevant' players
# now to subset this data for only looking at players that play significant minutes. We decide this.
cutoff <- 24.5
full_stats_qual <- subset(full_stats_qual, MPG >= cutoff)
#This full Df will have plenty of NA values, because the misc dataset has qualifiers that filter out irrelevant players based on min, pts, ast, stl, blk, etc.
full_stats_qual <- full_stats %>% drop_na()
full_stats_qual <- subset(full_stats_qual, MPG >= cutoff)
full_stats %>% drop_na() %>% sort(desc(MPG))
full_stats %>% drop_na() %>% select(MPG)
full_stats %>% drop_na() %>% select(Player, MPG) %>% desc()
full_stats %>% drop_na() %>% select(Player, MPG) %>% sort(MPG, decreasing =TRUE)
full_stats %>% drop_na() %>% select(Player, MPG) %>% sort(full_stats$MPG, decreasing =TRUE)
full_stats %>% drop_na() %>% select(Player, MPG) %>% summarize(mean(MPG))
install.packages("BBmisc")
library(BBmisc)
## Normalize data? So values with large ranges like pts don't have more weight than small range values like blk/stl
# Use BBmisc::normalize to (for each value) subtract the mean of the feature-range from that value and divide by the SD of the feature
normalized_data <- normalize(full_stats_qual, method = "standardize")
View(normalized_data)
?normalize
o=corrplot(cor(normalized_data),method='number')
corrplot(cor(normalized_data),method='number')
install.packages("corrplot")
corrplot(cor(normalized_data),method='number')
a <- cor(normalized_data)
mtcars
corrplot::corrplot(cor(normalized_data), method = "circle")
View(normalized_data)
?cor
normalized_data %>% select(-Player, -Pos, -Tm)
normalized_data %>% select(-Player, -Pos, -Tm) %>% corrplot::corrplot(cor(), method = "circle")
normalized_data %>% select(-Player, -Pos, -Tm) %>% corrplot::corrplot(cor(.), method = "circle")
corr_tbl <- normalized_data %>% select(-Player, -Pos, -Tm)
corrplot::corrplot(cor(corr_tbl))
corrplot::corrplot(cor(corr_tbl), method = "circle")
# PCoA
# covariance matrix creation
cov_matrix <- cov(normalized_data)
normalized_data <- normalized_data[c(7:65)]
# PCoA
# covariance matrix creation
cov_matrix <- cov(normalized_data)
# PCoA
# covariance matrix creation
cov_matrix <- cov(normalized_data) %>% round(3)
# find eigenvectors
eigenvectors <- eigen(cov_matrix)$vectors
# select first 2 eigenvectors
eigenv2 <- eigenvectors %>% select(c(1:2))
# select first 2 eigenvectors
eigenv2 <- eigenvectors[c(1:2)]
## Normalize data? So values with large ranges like pts don't have more weight than small range values like blk/stl
# Use BBmisc::normalize to (for each value) subtract the mean of the feature-range from that value and divide by the SD of the feature
normalized_numeric <- normalize(full_stats_qual, method = "standardize")
normalized_numeric <- normalized_data[c(7:65)]
normalized_numeric <- normalized_numeric[c(7:65)]
# PCoA
# covariance matrix creation
cov_matrix <- cov(normalized_numeric) %>% round(3)
# select first 2 eigenvectors
eigenv2 <- eigenvectors[,(1:2)]
as_tibble(normalized_numeric)
# convert to matrix for ordination
statsmatrix <- tibble(normalized_numeric)
# convert to matrix for ordination
statsmatrix <- tibble(normalized_numeric, rownames = NULL)
# convert to matrix for ordination
statsmatrix <- tibble(normalized_numeric, rownames = NA)
View(statsmatrix)
# convert to matrix for ordination
statsmatrix <- tibble(normalized_numeric)
View(statsmatrix)
